= [[TransactionalWrite]] TransactionalWrite

`TransactionalWrite` is an <<contract, abstraction>> of <<implementations, optimistic transactional writers>> that can <<writeFiles, write files out>>.

[[implementations]]
NOTE: <<OptimisticTransaction.adoc#, OptimisticTransaction>> is the default and only known `TransactionalWrite` in Delta Lake (indirectly as a concrete <<OptimisticTransactionImpl.adoc#, OptimisticTransactionImpl>>).

[[contract]]
.TransactionalWrite Contract (Abstract Methods Only)
[cols="30m,70",options="header",width="100%"]
|===
| Method
| Description

| deltaLog
a| [[deltaLog]]

[source, scala]
----
deltaLog: DeltaLog
----

Used when...FIXME

| protocol
a| [[protocol]]

[source, scala]
----
protocol: Protocol
----

Used when...FIXME

| snapshot
a| [[snapshot]]

[source, scala]
----
snapshot: Snapshot
----

Used when...FIXME

| metadata
a| [[metadata]]

[source, scala]
----
metadata: Metadata
----

Used when...FIXME

|===

== [[writeFiles]] Writing Files Out -- `writeFiles` Method

[source, scala]
----
writeFiles(
  data: Dataset[_]): Seq[AddFile]
writeFiles(
  data: Dataset[_],
  writeOptions: Option[DeltaOptions]): Seq[AddFile]
writeFiles(
  data: Dataset[_],
  isOptimize: Boolean): Seq[AddFile]
writeFiles(
  data: Dataset[_],
  writeOptions: Option[DeltaOptions],
  isOptimize: Boolean): Seq[AddFile]
----

`writeFiles` creates a <<DeltaInvariantCheckerExec.adoc#, DeltaInvariantCheckerExec>> and a <<DelayedCommitProtocol.adoc#, DelayedCommitProtocol>> to write out files (using `FileFormatWriter.write`) to the <<DeltaLog.adoc#dataPath, data path>> (of the <<deltaLog, DeltaLog>>).

`writeFiles` is executed within `SQLExecution.withNewExecutionId`.

NOTE: `writeFiles` can be tracked using web UI or `SQLAppStatusListener` (using `SparkListenerSQLExecutionStart` and `SparkListenerSQLExecutionEnd` events).

In the end, `writeFiles` returns the <<DelayedCommitProtocol.adoc#addedStatuses, addedStatuses>> of the `DelayedCommitProtocol` committer.

Internally, `writeFiles` turns the <<hasWritten, hasWritten>> flag on (`true`).

NOTE: After `writeFiles`, no <<OptimisticTransactionImpl.adoc#updateMetadata-AssertionError-hasWritten, metadata updates>> in the transaction are permitted.

`writeFiles` <<normalizeData, normalize>> the given `data` dataset (based on the <<Metadata.adoc#partitionColumns, partitionColumns>> of the <<OptimisticTransactionImpl.adoc#metadata, Metadata>>).

`writeFiles` <<getPartitioningColumns, getPartitioningColumns>> based on the <<Metadata.adoc#partitionSchema, partitionSchema>> of the <<OptimisticTransactionImpl.adoc#metadata, Metadata>>.

[[writeFiles-committer]]
`writeFiles` <<getCommitter, creates a DelayedCommitProtocol committer>> for the <<DeltaLog.adoc#dataPath, data path>> of the <<deltaLog, DeltaLog>>.

`writeFiles` <<Invariants.adoc#getFromSchema, gets the invariants>> from the <<Metadata.adoc#schema, schema>> of the <<OptimisticTransactionImpl.adoc#metadata, Metadata>>.

[[writeFiles-DeltaInvariantCheckerExec]][[writeFiles-FileFormatWriter]]
`writeFiles` requests a new Execution ID (that is used to track all Spark jobs of `FileFormatWriter.write` in Spark SQL) with a physical query plan of a new <<DeltaInvariantCheckerExec.adoc#, DeltaInvariantCheckerExec>> unary physical operator (with the executed plan of the normalized query execution as the child operator).

[NOTE]
====
`writeFiles` is used when:

* <<DeleteCommand.adoc#, DeleteCommand>>, <<MergeIntoCommand.adoc#, MergeIntoCommand>>, <<UpdateCommand.adoc#, UpdateCommand>>, and <<WriteIntoDelta.adoc#, WriteIntoDelta>> are requested to run

* `DeltaSink` is requested to <<DeltaSink.adoc#addBatch, addBatch>>
====

== [[getCommitter]] `getCommitter` Method

[source, scala]
----
getCommitter(
  outputPath: Path): DelayedCommitProtocol
----

`getCommitter` creates a new <<DelayedCommitProtocol.adoc#, DelayedCommitProtocol>> with the *delta* job ID and the given `outputPath` (and no random prefix).

NOTE: `getCommitter` is used exclusively when `TransactionalWrite` is requested to <<writeFiles, write files out>>.

== [[makeOutputNullable]] `makeOutputNullable` Method

[source, scala]
----
makeOutputNullable(output: Seq[Attribute]): Seq[Attribute]
----

`makeOutputNullable`...FIXME

NOTE: `makeOutputNullable` is used when...FIXME

== [[normalizeData]] `normalizeData` Method

[source, scala]
----
normalizeData(
  data: Dataset[_],
  partitionCols: Seq[String]): (QueryExecution, Seq[Attribute])
----

`normalizeData`...FIXME

NOTE: `normalizeData` is used when...FIXME

== [[getPartitioningColumns]] `getPartitioningColumns` Method

[source, scala]
----
getPartitioningColumns(
  partitionSchema: StructType,
  output: Seq[Attribute],
  colsDropped: Boolean): Seq[Attribute]
----

`getPartitioningColumns`...FIXME

NOTE: `getPartitioningColumns` is used when...FIXME

== [[hasWritten]] `hasWritten` Flag

[source, scala]
----
hasWritten: Boolean = false
----

`TransactionalWrite` uses the `hasWritten` internal registry to prevent `OptimisticTransactionImpl` from <<OptimisticTransactionImpl.adoc#updateMetadata, updating metadata>> after <<writeFiles, having written out any files>>.

`hasWritten` is initially turned off (`false`). It can be turned on (`true`) when `TransactionalWrite` is requested to <<writeFiles, write files out>>.
