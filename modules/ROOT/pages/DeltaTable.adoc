= [[DeltaTable]] DeltaTable Utility

`DeltaTable` is...FIXME

[source]
----
import io.delta.tables.DeltaTable.convertToDelta

assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession])
val tableId = "parquet.`/path`"
val partitionSchema = "key1 LONG, key2 STRING"

convertToDelta(spark, tableId, partitionSchema)
----

== [[forPath]] Creating DeltaTable -- `forPath` Utility

[source, scala]
----
forPath(
  path: String): DeltaTable // <1>
forPath(
  sparkSession: SparkSession,
  path: String): DeltaTable
----
<1> Uses `SparkSession.getActiveSession`

`forPath` creates a <<DeltaTable, DeltaTable>> for data in the given directory (`path`).

Internally, `forPath` makes sure that the given <<DeltaTableUtils.adoc#isDeltaTable, directory is part of a Delta table>> (as the root or a child directory).

NOTE: `forPath` is used when...FIXME

== [[convertToDelta]] Importing Parquet Table Into Delta Lake (Converting Parquet Table To Delta Format) -- `convertToDelta` Utility

[source, scala]
----
convertToDelta(
  spark: SparkSession,
  identifier: String,
  partitionSchema: StructType): DeltaTable
convertToDelta(
  spark: SparkSession,
  identifier: String,
  partitionSchema: String): DeltaTable  // <1>
convertToDelta(
  spark: SparkSession,
  identifier: String): DeltaTable
----
<1> Creates `StructType` from the given DDL-formatted `partitionSchema` string

`convertToDelta` requests the `SparkSession` for the SQL parser (`ParserInterface`) that is in turn requested to parse the given table identifier (to get a `TableIdentifier`).

TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-ParserInterface.html[ParserInterface] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book.

In the end, `convertToDelta` uses the `DeltaConvert` utility to <<DeltaConvert.adoc#executeConvert, convert the parquet table to delta format>> and <<forPath, creates a delta table>>.
