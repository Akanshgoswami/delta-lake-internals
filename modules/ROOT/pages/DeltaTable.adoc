= [[DeltaTable]] DeltaTable

`DeltaTable` is...FIXME

[source]
----
import io.delta.tables.{DeltaTable => delta}

assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession])
val tableId = "parquet.`/tmp/delta-lake`"
val partitionSchema = "key1 LONG, key2 STRING"

delta.convertToDelta(spark, tableId, partitionSchema)
----

== [[forPath]] Creating DeltaTable -- `DeltaTable.forPath` Utility

[source, scala]
----
forPath(
  path: String): DeltaTable // <1>
forPath(
  sparkSession: SparkSession,
  path: String): DeltaTable
----
<1> Uses `SparkSession.getActiveSession`

`forPath` creates a <<DeltaTable, DeltaTable>> for data in the given directory (`path`) only when the given <<DeltaTableUtils.adoc#isDeltaTable, directory is part of a delta table>> (as the root or a child directory).

`forPath` throws an `AnalysisException` when the given `path` does not belong to a delta table:

```
[deltaTableIdentifier] is not a Delta table.
```

Internally, `forPath` creates a new <<DeltaTable, DeltaTable>> with the following:

* `Dataset` that represents loading data from the specified `path` using <<DeltaDataSource.adoc#delta-format, delta>> data source

* <<DeltaLog.adoc#, DeltaLog>> for the <<DeltaLog.adoc#forTable, table in the specified path>>

NOTE: `forPath` is used when `DeltaTable` utility is used to <<convertToDelta, convert a parquet table to delta format (DeltaTable.convertToDelta)>>.

== [[convertToDelta]] Converting Parquet Table To Delta Format (Importing Parquet Table Into Delta Lake) -- `DeltaTable.convertToDelta` Utility

[source, scala]
----
convertToDelta(
  spark: SparkSession,
  identifier: String,
  partitionSchema: StructType): DeltaTable
convertToDelta(
  spark: SparkSession,
  identifier: String,
  partitionSchema: String): DeltaTable  // <1>
convertToDelta(
  spark: SparkSession,
  identifier: String): DeltaTable
----
<1> Creates `StructType` from the given DDL-formatted `partitionSchema` string

`convertToDelta` requests the `SparkSession` for the SQL parser (`ParserInterface`) that is in turn requested to parse the given table identifier (to get a `TableIdentifier`).

TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-ParserInterface.html[ParserInterface] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book.

In the end, `convertToDelta` uses the `DeltaConvert` utility to <<DeltaConvert.adoc#executeConvert, convert the parquet table to delta format>> and <<forPath, creates a DeltaTable>>.
