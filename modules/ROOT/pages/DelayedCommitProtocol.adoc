= [[DelayedCommitProtocol]] DelayedCommitProtocol

`DelayedCommitProtocol` is a concrete `FileCommitProtocol` (Spark Core) to write out partitions to a <<path, directory>> and return a <<addedStatuses, list of files added>>.

TIP: Read up on https://books.japila.pl/apache-spark-internals/apache-spark-internals/2.4.4/spark-internal-io-FileCommitProtocol.html[FileCommitProtocol] in https://books.japila.pl/apache-spark-internals[The Internals Of Apache Spark] online book.

`DelayedCommitProtocol` is <<creating-instance, created>> exclusively when `TransactionalWrite` is requested for a <<TransactionalWrite.adoc#getCommitter, committer to write partitions>> to the <<path, directory>>.

[[logging]]
[TIP]
====
Enable `ALL` logging level for `org.apache.spark.sql.delta.files.DelayedCommitProtocol` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.delta.files.DelayedCommitProtocol=ALL
```

Refer to <<logging.adoc#, Logging>>.
====

== [[creating-instance]] Creating DelayedCommitProtocol Instance

`DelayedCommitProtocol` takes the following to be created:

* [[jobId]] Job ID (always <<TransactionalWrite.adoc#getCommitter, delta>>)
* [[path]] Directory
* [[randomPrefixLength]] Optional length of a random prefix (always <<TransactionalWrite.adoc#getCommitter, empty>>)

`DelayedCommitProtocol` initializes the <<internal-properties, internal properties>>.

== [[setupTask]] `setupTask` Method

[source, scala]
----
setupTask(
  taskContext: TaskAttemptContext): Unit
----

NOTE: `setupTask` is part of the `FileCommitProtocol` contract to set up a task for a writing job.

`setupTask` simply initializes the <<addedFiles, addedFiles>> internal registry to be empty.

== [[newTaskTempFile]] `newTaskTempFile` Method

[source, scala]
----
newTaskTempFile(
  taskContext: TaskAttemptContext,
  dir: Option[String],
  ext: String): String
----

NOTE: `newTaskTempFile` is part of the `FileCommitProtocol` contract to inform the commit protocol to add a new file.

`newTaskTempFile` <<getFileName, creates a file name>> for the given `TaskAttemptContext` and `ext`.

`newTaskTempFile` tries to <<parsePartitions, parsePartitions>> with the given `dir` or falls back to an empty `partitionValues`.

NOTE: The given `dir` defines a partition directory if it is partitioned. (FIXME What's the `it` part?)

`newTaskTempFile` builds a path (based on the given `randomPrefixLength` and the `dir`, or uses the file name directly).

NOTE: FIXME When would the optional `dir` and the <<randomPrefixLength, randomPrefixLength>> be defined?

`newTaskTempFile` adds the partition values and the relative path to the <<addedFiles, addedFiles>> internal registry.

In the end, `newTaskTempFile` returns the absolute path of the (relative) path in the <<path, directory>>.

== [[commitTask]] `commitTask` Method

[source, scala]
----
commitTask(
  taskContext: TaskAttemptContext): TaskCommitMessage
----

NOTE: `commitTask` is part of the `FileCommitProtocol` contract to commit a task after the writes succeed.

`commitTask` returns an empty `TaskCommitMessage` when the <<addedFiles, addedFiles>> internal registry has no entries (<<newTaskTempFile, files added>>).

`commitTask` creates a `TaskCommitMessage` with an <<AddFile.adoc#, AddFile>> for every <<newTaskTempFile, file added>> by tasks on executors (`Seq[AddFile]`).

== [[commitJob]] Committing Spark Job (After Writes Succeed) -- `commitJob` Method

[source, scala]
----
commitJob(
  jobContext: JobContext,
  taskCommits: Seq[TaskCommitMessage]): Unit
----

NOTE: `commitJob` is part of the `FileCommitProtocol` contract to commit a job after the writes succeed.

`commitJob` simply adds the <<AddFile.adoc#, AddFiles>> (from the given `taskCommits`) to the <<addedStatuses, addedStatuses>> internal registry.

== [[parsePartitions]] `parsePartitions` Method

[source, scala]
----
parsePartitions(
  dir: String): Map[String, String]
----

`parsePartitions`...FIXME

NOTE: `parsePartitions` is used exclusively when `DelayedCommitProtocol` is requested to <<newTaskTempFile, newTaskTempFile>>.

== [[setupJob]] `setupJob` Method

[source, scala]
----
setupJob(
  jobContext: JobContext): Unit
----

NOTE: `setupJob` is part of the `FileCommitProtocol` contract to set up a Spark job.

`setupJob` does nothing.

== [[abortJob]] `abortJob` Method

[source, scala]
----
abortJob(
  jobContext: JobContext): Unit
----

NOTE: `abortJob` is part of the `FileCommitProtocol` contract to abort a Spark job.

`abortJob` does nothing.

== [[getFileName]] `getFileName` Method

[source, scala]
----
getFileName(
  taskContext: TaskAttemptContext,
  ext: String): String
----

`getFileName` takes the task ID from the given `TaskAttemptContext` (for the `split` part below).

`getFileName` generates a random UUID (for the `uuid` part below).

In the end, `getFileName` returns a file name of the format:

```
part-[split]%05d-[uuid][ext]
```

NOTE: `getFileName` is used exclusively when `DelayedCommitProtocol` is requested to <<newTaskTempFile, newTaskTempFile>>.

== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| addedFiles
a| [[addedFiles]]

[source, scala]
----
addedFiles: ArrayBuffer[(Map[String, String], String)]
----

Tracks the list of files <<newTaskTempFile, added by tasks>> (on executors)

Initialized (as an empty collection) in <<setupTask, setupTask>>

Used exclusively when `DelayedCommitProtocol` is requested to <<commitTask, commit a task>>

|===
