= DeltaSQLConf -- spark.databricks.delta Configuration Properties

The following table contains `spark.databricks.delta.`-prefixed configuration properties.

.spark.databricks.delta Configuration Properties
[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| import.batchSize.schemaInference
a| [[import.batchSize.schemaInference]][[DELTA_IMPORT_BATCH_SIZE_SCHEMA_INFERENCE]] **(internal)** Number of files per batch for schema inference during <<ConvertToDeltaCommand.adoc#performConvert-schemaBatchSize, import>>.

Default: `1000000`

| import.batchSize.statsCollection
a| [[import.batchSize.statsCollection]][[DELTA_IMPORT_BATCH_SIZE_STATS_COLLECTION]] **(internal)** Number of files per batch for stats collection during <<ConvertToDeltaCommand.adoc#performConvert-schemaBatchSize, import>>.

Default: `50000`

| schema.autoMerge.enabled
a| [[schema.autoMerge.enabled]][[DELTA_SCHEMA_AUTO_MIGRATE]] Enables schema merging on appends and overwrites.

Equivalent DataFrame option: <<DeltaOptions.adoc#mergeSchema, mergeSchema>>

Default: `false`

| snapshotPartitions
a| [[snapshotPartitions]][[DELTA_SNAPSHOT_PARTITIONS]] **(internal)** Number of partitions to use for *state reconstruction* (when <<Snapshot.adoc#stateReconstruction, building>> a <<Snapshot.adoc#, snapshot>> of a delta table).

Default: `50`

| timeTravel.resolveOnIdentifier.enabled
a| [[timeTravel.resolveOnIdentifier.enabled]] **(internal)** When enabled (`true`), patterns as `@v123` in identifiers are considered as <<time-travel.adoc#, time travel>> nodes.

Default: `true`

|===

== [[commitInfo.enabled]][[DELTA_COMMIT_INFO_ENABLED]] commitInfo.enabled

*spark.databricks.delta.commitInfo.enabled* controls whether to xref:OptimisticTransactionImpl.adoc#commitInfo[log commit information into a Delta log].

Default: `true`
