== The Internals of Delta Lake {page-component-version}

https://delta.io/[Delta Lake] is an open-source storage layer that brings ACID transactions to https://spark.apache.org/[Apache Spark] and big data workloads.

Delta Lake uses a Hadoop DFS-compliant file system for the underlying storage.

Technically speaking, Delta Lake is:

. <<data-source, Data Source for Spark SQL and Structured Streaming>>

. <<streaming-sink, Streaming sink for streaming queries (Structured Streaming)>>

=== [[data-source]] Data Source for Spark SQL and Structured Streaming

Delta Lake is a data source for Spark SQL and Structured Streaming (see https://github.com/delta-io/delta/blob/v0.4.0/src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala#L40-L45[github]).

[source, scala]
----
val input = spark
  .read // or readStream
  .format("delta")
  .load
----

In that sense, Delta Lake is like `parquet`, `kafka` or any data source that can be used in batch and streaming queries and is simply an extension of Apache Spark's Spark SQL and Structured Streaming.

As a `DataSourceRegister`, Delta Lake registers itself as `delta` data source.

TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceRegister.html[DataSourceRegister] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book.

In order to "install" and use Delta Lake in a Spark application (e.g. `spark-shell`), use `--packages` command-line option.

[source, scala]
----
// $ spark-shell --packages io.delta:delta-core_2.12:0.4.0

assert(spark.version == "2.4.4")

val input = spark
  .read
  .format("delta")
  .option("path", "delta") // a required option
  .load
----

`delta` data source requires some <<options.adoc#, options>> (with <<options.adoc#path, path>> option required).

=== [[streaming-sink]] Streaming Sink for Streaming Queries (Structured Streaming)

Delta Lake is a <<DeltaDataSource.adoc#StreamSinkProvider, streaming sink>> for streaming queries in Structured Streaming.

[source, scala]
----
// $ spark-shell --packages io.delta:delta-core_2.12:0.4.0

assert(spark.version == "2.4.4")

// Any streaming query would work
// Using rate data source for demo
// val rates = spark.readStream.format("rate").load

val q = rates
  .writeStream
  .format("delta")
  .option("checkpointLocation", "delta-checkpointLocation") // a required option
  .option("path", "delta") // a required option
  .start
----
