= The Internals of Delta Lake {page-component-version}

https://delta.io/[Delta Lake] is an open-source storage management system (storage layer) that brings ACID transactions and time travel to https://spark.apache.org/[Apache Spark] and big data workloads.

Delta Lake introduces a concept of <<DeltaTable.adoc#, delta table>> that is simply a <<DeltaFileFormat.adoc#fileFormat, parquet table>> with a <<DeltaLog.adoc#, transactional log>>.

Changes to (the state of) a delta table are reflected as <<Action.adoc#, actions>> and persisted to the transactional log (in <<Action.adoc#json, JSON format>>).

Delta Lake uses <<OptimisticTransaction.adoc#, OptimisticTransaction>> for <<TransactionalWrite.adoc#, transactional writes>>. A <<OptimisticTransactionImpl.adoc#commit, commit>> is successful when the transaction can <<OptimisticTransactionImpl.adoc#doCommit-write, write>> the actions to a delta file (in the <<DeltaLog.adoc#, transactional log>>). In case the delta file for the commit version already exists, the transaction is <<OptimisticTransactionImpl.adoc#checkAndRetry, retried>>.

Structured queries can write (transactionally) to a delta table using the following interfaces:

* <<WriteIntoDelta.adoc#, WriteIntoDelta>> command for batch queries (Spark SQL)

* <<DeltaSink.adoc#, DeltaSink>> for streaming queries (Spark Structured Streaming)

More importantly, multiple queries can write to the same delta table simultaneously (at exactly the same time).

Delta Lake provides <<DeltaTable.adoc#, DeltaTable API>> to programmatically access Delta tables. A delta table can be created <<DeltaTable.adoc#convertToDelta, based on a parquet table (DeltaTable.convertToDelta)>> or <<DeltaTable.adoc#forPath, from scratch (DeltaTable.forPath)>>.

Delta Lake supports Spark SQL and Structured Streaming using <<DeltaDataSource.adoc#DataSourceRegister, delta>> format.

Delta Lake supports reading and writing in batch queries:

* <<DeltaDataSource.adoc#RelationProvider, Batch reads>> (as a `RelationProvider`)

* <<DeltaDataSource.adoc#CreatableRelationProvider, Batch writes>> (as a `CreatableRelationProvider`)

Delta Lake supports reading and writing in streaming queries:

* <<DeltaDataSource.adoc#StreamSourceProvider, Stream reads>> (as a `Source`)

* <<DeltaDataSource.adoc#StreamSinkProvider, Stream writes>> (as a `Sink`)

Delta Lake uses <<DeltaLog.adoc#store, LogStore>> abstraction to read and write physical log files and checkpoints (using https://hadoop.apache.org/docs/current2/hadoop-project-dist/hadoop-common/filesystem/index.html[Hadoop FileSystem API]).

== Installing Delta Lake

In order to "install" and use Delta Lake in a Spark application (e.g. `spark-shell`), use `--packages` command-line option.

[source, scala]
----
/*
./bin/spark-shell \
  --packages io.delta:delta-core_2.12:0.5.0 \
  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
*/
assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession])
assert(spark.version.matches("2.4.[2-4]"), "Delta Lake supports Spark 2.4.2+")

val input = spark
  .read
  .format("delta")
  .option("path", "delta")
  .load
----

`delta` data source requires some <<DeltaOptions.adoc#, options>> (with <<options.adoc#path, path>> option required).
