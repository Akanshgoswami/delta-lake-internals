= The Internals of Delta Lake {page-component-version}

https://delta.io/[Delta Lake] is an open-source storage management system (storage layer) that brings ACID transactions and time travel to https://spark.apache.org/[Apache Spark] and big data workloads.

Delta Lake introduces a concept of *delta table* that is simply a parquet table with <<DeltaLog.adoc#, transactional log of changes>>.

Changes to (the state of) a Delta table are reflected as <<Action.adoc#, actions>> and persisted to the log (in <<Action.adoc#json, JSON format>>).

Delta Lake uses <<OptimisticTransaction.adoc#, OptimisticTransaction>> for <<TransactionalWrite.adoc#, transactional writes>>.

Delta Lake provides <<DeltaTable.adoc#, DeltaTable API>> to programmatically access Delta tables. A delta table can be created <<DeltaTable.adoc#convertToDelta, based on a parquet table (DeltaTable.convertToDelta)>> or <<DeltaTable.adoc#forPath, from scratch (DeltaTable.forPath)>>.

Delta Lake supports Spark SQL and Structured Streaming using <<DeltaDataSource.adoc#DataSourceRegister, delta>> format.

Delta Lake supports reading and writing in batch queries:

* <<DeltaDataSource.adoc#RelationProvider, Batch reads>> (as a `RelationProvider`)

* <<DeltaDataSource.adoc#CreatableRelationProvider, Batch writes>> (as a `CreatableRelationProvider`)

Delta Lake supports reading and writing in streaming queries:

* <<DeltaDataSource.adoc#StreamSourceProvider, Streaming reads>> (as a `Source`)

* <<DeltaDataSource.adoc#StreamSinkProvider, Streaming writes>> (as a `Sink`)

Delta Lake uses https://hadoop.apache.org/docs/current2/hadoop-project-dist/hadoop-common/filesystem/index.html[Hadoop FileSystem API] to access the underlying data storage (e.g. http://hadoop.apache.org/[Hadoop DFS] or https://hadoop.apache.org/docs/current2/hadoop-aws/tools/hadoop-aws/index.html[Amazon S3]).

== Installing Delta Lake

In order to "install" and use Delta Lake in a Spark application (e.g. `spark-shell`), use `--packages` command-line option.

[source, scala]
----
/*
./bin/spark-shell \
  --packages io.delta:delta-core_2.12:0.4.0 \
  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
*/
assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession])
assert(spark.version.matches("2.4.[2-4]"), "Delta Lake supports Spark 2.4.2+")

val input = spark
  .read
  .format("delta")
  .option("path", "delta")
  .load
----

`delta` data source requires some <<options.adoc#, options>> (with <<options.adoc#path, path>> option required).
