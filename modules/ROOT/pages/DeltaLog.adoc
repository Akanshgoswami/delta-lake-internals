= [[DeltaLog]] DeltaLog

`DeltaLog` is a transactional log of changes (actions) to the state of a <<dataPath, delta table>>.

`DeltaLog` is <<creating-instance, created>> (indirectly via <<apply, DeltaLog.apply>> utility) when <<forTable, DeltaLog.forTable>> utility is used.

[source, scala]
----
import org.apache.spark.sql.SparkSession
assert(spark.isInstanceOf[SparkSession])

import org.apache.spark.sql.delta.DeltaLog
val deltaLog = DeltaLog.forTable(spark, dataPath = "/tmp/delta/t1")

scala> println(deltaLog.LAST_CHECKPOINT)
file:/tmp/delta/t1/_delta_log/_last_checkpoint
----

`DeltaLog` is a <<LogStoreProvider.adoc#, LogStoreProvider>>.

== [[_delta_log]] _delta_log Metadata Directory

`DeltaLog` uses *_delta_log* metadata directory under the <<dataPath, data path>> directory (that is specified using <<forTable, DeltaLog.forTable>> utility).

The `_delta_log` directory is resolved (in the <<apply, DeltaLog.apply>> utility) using the application-wide Hadoop https://hadoop.apache.org/docs/current2/api/org/apache/hadoop/conf/Configuration.html[Configuration].

[NOTE]
====
<<apply, DeltaLog.apply>> utility uses the given `SparkSession` to create an Hadoop `Configuration` instance.

[source, scala]
----
spark.sessionState.newHadoopConf()
----
====

Once resolved and turned into a qualified path, the `_delta_log` directory for the delta table (under the <<dataPath, data path>> directory) is <<deltaLogCache, cached>> for later reuse.

== [[store]] LogStore

`DeltaLog` uses an <<LogStore.adoc#, LogStore>> for...FIXME

== [[deltaLogCache]] `deltaLogCache` Cache of `_delta_log` Fully-Qualified Paths

[source, scala]
----
deltaLogCache: Cache[Path, DeltaLog]
----

`deltaLogCache` is a cache of <<DeltaLog, DeltaLogs>> by their fully-qualified <<_delta_log, _delta_log>> metadata directories.

`deltaLogCache` has a new `DeltaLog` added in <<apply, DeltaLog.apply>> utility.

`deltaLogCache` is invalidated for a `Path` in <<apply, DeltaLog.apply>>, <<invalidateCache, DeltaLog.invalidateCache>> utilities and all `Paths` in <<clearCache, DeltaLog.clearCache>> utility.

== [[creating-instance]] Creating DeltaLog Instance

`DeltaLog` takes the following to be created:

* [[logPath]] Log directory (Hadoop https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/Path.html[Path])
* [[dataPath]] Data directory (Hadoop https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/Path.html[Path])
* [[clock]] `Clock`

`DeltaLog` initializes the <<internal-properties, internal properties>>.

== [[apply]] Creating DeltaLog Instance -- `apply` Utility

[source, scala]
----
apply(
  spark: SparkSession,
  rawPath: Path,
  clock: Clock = new SystemClock): DeltaLog
----

`apply`...FIXME

NOTE: `apply` is used when `DeltaLog` is requested to <<forTable, forTable>>.

== [[withNewTransaction]] Executing Single-Threaded Operation (Block) in New Transaction -- `withNewTransaction` Method

[source, scala]
----
withNewTransaction[T](
  thunk: OptimisticTransaction => T): T
----

`withNewTransaction` <<startTransaction, starts a new transaction>> (that is <<OptimisticTransaction.adoc#setActive, active>> for the whole thread) and executes the given `thunk` block.

In the end, `withNewTransaction` makes the transaction <<OptimisticTransaction.adoc#clearActive, no longer active>>.

[NOTE]
====
`withNewTransaction` is used when:

* <<DeleteCommand.adoc#, DeleteCommand>>, <<MergeIntoCommand.adoc#, MergeIntoCommand>>, <<UpdateCommand.adoc#, UpdateCommand>>, and <<WriteIntoDelta.adoc#, WriteIntoDelta>> logical commands are executed

* `DeltaSink` is requested to <<DeltaSink.adoc#addBatch, addBatch>>
====

== [[startTransaction]] Starting New Transaction -- `startTransaction` Method

[source, scala]
----
startTransaction(): OptimisticTransaction
----

`startTransaction` <<update, updates>> and creates a new <<OptimisticTransaction.adoc#, OptimisticTransaction>> (for this `DeltaLog`).

NOTE: `startTransaction` is a subset of <<withNewTransaction, withNewTransaction>>.

[NOTE]
====
`startTransaction` is used when:

* `DeltaLog` is requested to <<upgradeProtocol, upgradeProtocol>>

* <<ConvertToDeltaCommand.adoc#, ConvertToDeltaCommand>> is executed
====

== [[assertRemovable]] Throwing UnsupportedOperationException For appendOnly Table Property Enabled -- `assertRemovable` Method

[source, scala]
----
assertRemovable(): Unit
----

`assertRemovable` throws an `UnsupportedOperationException` for the <<DeltaConfigs.adoc#IS_APPEND_ONLY, appendOnly>> table property (<<DeltaConfigs.adoc#fromMetaData, in>> the <<metadata, Metadata>>) enabled (`true`):

```
This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE <table_name> SET TBLPROPERTIES (appendOnly=false)'.
```

NOTE: `assertRemovable` is used when...FIXME

== [[metadata]] `metadata` Method

[source, scala]
----
metadata: Metadata
----

NOTE: `metadata` is part of the <<Checkpoints.adoc#metadata, Checkpoints Contract>> to...FIXME.

`metadata` requests the <<snapshot, current Snapshot>> for the <<Snapshot.adoc#metadata, metadata>> or creates a new <<Metadata.adoc#, Metadata>> (if the <<snapshot, current Snapshot>> is not initialized).

== [[forTable]] Creating DeltaLog Instance -- `forTable` Utility

[source, scala]
----
forTable(
  spark: SparkSession,
  dataPath: File): DeltaLog
forTable(
  spark: SparkSession,
  dataPath: File,
  clock: Clock): DeltaLog
forTable(
  spark: SparkSession,
  dataPath: Path): DeltaLog
forTable(
  spark: SparkSession,
  dataPath: Path,
  clock: Clock): DeltaLog
forTable(
  spark: SparkSession,
  dataPath: String): DeltaLog
forTable(
  spark: SparkSession,
  dataPath: String,
  clock: Clock): DeltaLog
----

`forTable` creates a <<apply, DeltaLog>> with *_delta_log* directory (in the given `dataPath` directory).

[NOTE]
====
`forTable` is used when:

* <<DeltaTable.adoc#forPath, DeltaTable.forPath>> utility is used to create a <<DeltaTable.adoc#, DeltaTable>>

* <<ConvertToDeltaCommand.adoc#, ConvertToDeltaCommand>>, <<DescribeDeltaHistoryCommand.adoc#, DescribeDeltaHistoryCommand>>, <<VacuumTableCommand.adoc#, VacuumTableCommand>> are requested to `run`

* `DeltaDataSource` is requested to <<DeltaDataSource.adoc#sourceSchema, sourceSchema>>, <<DeltaDataSource.adoc#createSource, createSource>>, and create a relation (as <<DeltaDataSource.adoc#CreatableRelationProvider-createRelation, CreatableRelationProvider>> and <<DeltaDataSource.adoc#RelationProvider-createRelation, RelationProvider>>)

* <<DeltaTableUtils.adoc#combineWithCatalogMetadata, DeltaTableUtils.combineWithCatalogMetadata>> utility is used

* `DeltaTableIdentifier` is requested to `getDeltaLog`

* <<DeltaSink.adoc#, DeltaSink>> is created
====

== [[update]] `update` Method

[source, scala]
----
update(
  stalenessAcceptable: Boolean = false): Snapshot
----

`update` branches off based on a combination of flags: the given `stalenessAcceptable` and <<isSnapshotStale, isSnapshotStale>> flags.

For the `stalenessAcceptable` not acceptable (default) and the <<isSnapshotStale, snapshot not stale>>, `update` simply acquires the <<deltaLogLock, deltaLogLock>> lock and <<updateInternal, updateInternal>> (with `isAsync` flag off).

For all other cases, `update`...FIXME

[NOTE]
====
`update` is used when:

* `DeltaHistoryManager` is requested to <<DeltaHistoryManager.adoc#getHistory, getHistory>>, <<DeltaHistoryManager.adoc#getActiveCommitAtTime, getActiveCommitAtTime>>, and <<DeltaHistoryManager.adoc#checkVersionExists, checkVersionExists>>

* `DeltaLog` is <<creating-instance, created>> (with no <<Checkpoints.adoc#lastCheckpoint, checkpoint>> created), and requested to <<startTransaction, startTransaction>> and <<withNewTransaction, withNewTransaction>>

* `OptimisticTransactionImpl` is requested to <<OptimisticTransactionImpl.adoc#doCommit, doCommit>> and <<OptimisticTransactionImpl.adoc#checkAndRetry, checkAndRetry>>

* `ConvertToDeltaCommand` is requested to <<ConvertToDeltaCommand.adoc#run, run>> and <<ConvertToDeltaCommand.adoc#streamWrite, streamWrite>>

* `VacuumCommand` utility is used to <<VacuumCommand.adoc#gc, gc>>

* `TahoeLogFileIndex` is requested for the <<TahoeLogFileIndex.adoc#getSnapshot, (historical or latest) snapshot>>

* `DeltaDataSource` is requested for a <<DeltaDataSource.adoc#RelationProvider-createRelation, relation>>
====

== [[snapshot]] Current Snapshot -- `snapshot` Method

[source, scala]
----
snapshot: Snapshot
----

`snapshot` returns the <<currentSnapshot, current snapshot>>.

NOTE: `snapshot` is used when...FIXME

== [[createRelation]] Creating Relation (per Partition Filters and Time Travel) -- `createRelation` Method

[source, scala]
----
createRelation(
  partitionFilters: Seq[Expression] = Nil,
  timeTravel: Option[DeltaTimeTravelSpec] = None): BaseRelation
----

`createRelation`...FIXME

`createRelation` creates a <<TahoeLogFileIndex.adoc#, TahoeLogFileIndex>> for the <<dataPath, data path>>, the given `partitionFilters` and a version (if defined).

`createRelation`...FIXME

In the end, `createRelation` creates a `HadoopFsRelation` for the `TahoeLogFileIndex` and...FIXME. The `HadoopFsRelation` is also an <<createRelation-InsertableRelation, InsertableRelation>>.

NOTE: `createRelation` is used when `DeltaDataSource` is requested to create a relation as a <<DeltaDataSource.adoc#CreatableRelationProvider, CreatableRelationProvider>> and a <<DeltaDataSource.adoc#RelationProvider, RelationProvider>>

=== [[createRelation-InsertableRelation]][[createRelation-InsertableRelation-insert]] `insert` Method

[source, scala]
----
insert(
  data: DataFrame,
  overwrite: Boolean): Unit
----

NOTE: `insert` is part of the `InsertableRelation` contract to...FIXME.

`insert`...FIXME

== [[getSnapshotAt]] `getSnapshotAt` Method

[source, scala]
----
getSnapshotAt(
  version: Long,
  commitTimestamp: Option[Long] = None,
  lastCheckpointHint: Option[CheckpointInstance] = None): Snapshot
----

`getSnapshotAt`...FIXME

[NOTE]
====
`getSnapshotAt` is used when:

* `DeltaLog` is requested for a <<createRelation, relation>>, and to <<updateInternal, updateInternal>>

* `DeltaSource` is requested to <<DeltaSource.adoc#getSnapshotAt, getSnapshotAt>>

* `TahoeLogFileIndex` is requested for <<TahoeLogFileIndex.adoc#historicalSnapshotOpt, historicalSnapshotOpt>>
====

== [[tryUpdate]] `tryUpdate` Method

[source, scala]
----
tryUpdate(
  isAsync: Boolean = false): Snapshot
----

`tryUpdate`...FIXME

NOTE: `tryUpdate` is used exclusively when `DeltaLog` is requested to <<update, update>>.

== [[ensureLogDirectoryExist]] `ensureLogDirectoryExist` Method

[source, scala]
----
ensureLogDirectoryExist(): Unit
----

`ensureLogDirectoryExist`...FIXME

NOTE: `ensureLogDirectoryExist` is used when...FIXME

== [[protocolWrite]] `protocolWrite` Method

[source, scala]
----
protocolWrite(
  protocol: Protocol,
  logUpgradeMessage: Boolean = true): Unit
----

`protocolWrite`...FIXME

NOTE: `protocolWrite` is used when...FIXME

== [[checkpointInterval]] `checkpointInterval` Method

[source, scala]
----
checkpointInterval: Int
----

`checkpointInterval` gives the value of <<DeltaConfigs.adoc#CHECKPOINT_INTERVAL, checkpointInterval>> table property (<<DeltaConfigs.adoc#fromMetaData, from>> the <<metadata, Metadata>>).

NOTE: `checkpointInterval` is used when...FIXME

== [[tombstoneRetentionMillis]] `tombstoneRetentionMillis` Method

[source, scala]
----
tombstoneRetentionMillis: Long
----

`tombstoneRetentionMillis` gives the value of <<DeltaConfigs.adoc#TOMBSTONE_RETENTION, deletedFileRetentionDuration>> table property (<<DeltaConfigs.adoc#fromMetaData, from>> the <<metadata, Metadata>>).

NOTE: `tombstoneRetentionMillis` is used when...FIXME

== [[getChanges]] `getChanges` Method

[source, scala]
----
getChanges(
  startVersion: Long): Iterator[(Long, Seq[Action])]
----

`getChanges`...FIXME

NOTE: `getChanges` is used when...FIXME

== [[upgradeProtocol]] `upgradeProtocol` Method

[source, scala]
----
upgradeProtocol(
  newVersion: Protocol = Protocol()): Unit
----

`upgradeProtocol`...FIXME

NOTE: `upgradeProtocol` is used when...FIXME

== [[currentSnapshot]] `currentSnapshot` Internal Registry

[source, scala]
----
currentSnapshot: Snapshot
----

`currentSnapshot`...FIXME

NOTE: `currentSnapshot` is used when...FIXME

== [[updateInternal]] `updateInternal` Internal Method

[source, scala]
----
updateInternal(isAsync: Boolean): Snapshot
----

`updateInternal`...FIXME

NOTE: `updateInternal` is used when `DeltaLog` is requested to <<update, update>> and <<tryUpdate, tryUpdate>>.

== [[invalidateCache]] Invalidating Cached DeltaLog Instance -- `invalidateCache` Utility

[source, scala]
----
invalidateCache(
  spark: SparkSession,
  dataPath: Path): Unit
----

`invalidateCache`...FIXME

NOTE: `invalidateCache` does not seem to be used at all.

== [[clearCache]] Removing (Clearing) All Cached DeltaLog Instances -- `clearCache` Utility

[source, scala]
----
clearCache(): Unit
----

`clearCache`...FIXME

NOTE: `clearCache` seems to be used exclusively in tests.

== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| deltaLogLock
a| [[deltaLogLock]] Lock

Used when...FIXME

|===
